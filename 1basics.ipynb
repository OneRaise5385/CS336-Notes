{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c775ad36",
   "metadata": {},
   "source": [
    "# 1 basics\n",
    "## 1.1 Tokenization\n",
    "\n",
    "    intro_to_tokenization()\n",
    "    tokenization_examples()\n",
    "    character_tokenizer()\n",
    "    byte_tokenizer()\n",
    "    word_tokenizer()\n",
    "    bpe_tokenizer()\n",
    "\n",
    "### 什么是Tokenization？\n",
    "原始文本通常用 Unicode 字符串表示。语言模型会对一系列**词元**（tokens）（通常用整数索引表示）放置一个概率分布。因此，我们需要一种将字符串**编码**（encode）成词元的过程，同时需要一种将词元**解码**（decode）回字符串的过程。Tokenizer（分词器）就是一个实现了编码和解码方法的类。**词表大小**（vocabulary size）就是可能的词元（整数）总数。\n",
    "\n",
    "为了直观了解分词器是如何工作的，可以试用这个[交互式网站](https://tiktokenizer.vercel.app/?encoder=gpt2)。\n",
    "\n",
    "观察到的现象：\n",
    "1. 一个单词及其前面的空格会被当作同一个 token（例如 \" world\"）。\n",
    "2. 处在句首的单词和处在句中间的同一单词会被不同地表示（例如 \"hello hello\"）。\n",
    "3. 数字会被分成每几位一个 token。\n",
    "\n",
    "以下是 OpenAI 的 GPT-2 分词器（tiktoken）的实际运行示例：\n",
    "\n",
    "python\n",
    "复制\n",
    "编辑\n",
    "tokenizer = get_gpt2_tokenizer()\n",
    "string = \"Hello, 🌍! 你好!\"  # @inspect string\n",
    "\n",
    "# 检查 encode() 和 decode() 是否能够无损往返\n",
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "assert string == reconstructed_string\n",
    "\n",
    "compression_ratio = get_compression_ratio(string, indices)  # @inspect compression_ratio\n",
    "这段意思是在演示如何用 GPT-2 分词器将字符串编码成 token，再解码回来，并验证它们是否完全一致，同时计算压缩比。\n",
    "\n",
    "### 基于字符的分词器\n",
    "\n",
    "### 基于字节的分词器\n",
    "\n",
    "### 基于单词的分词器\n",
    "\n",
    "### 基于BPE的分词器\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
