{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150df03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20272702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be395f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0).__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7040e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'this is a test' + chr(0) + 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a5a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print('this is a test' + chr(0) + 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6970aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "hello! こんにちは!\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(utf8_encoded.decode(\"utf-8\"))\n",
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150ec1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h 104\n",
      "e 101\n",
      "l 108\n",
      "l 108\n",
      "o 111\n",
      "! 33\n",
      "  32\n",
      "こ 227\n",
      "ん 129\n",
      "に 147\n",
      "ち 227\n",
      "は 130\n",
      "! 147\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_string)):\n",
    "    print(test_string[i], utf8_encoded[i])\n",
    "    \n",
    "print(list(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a28bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(test_string))\n",
    "print(len(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3de40c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并保存\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "ds.save_to_disk('../data/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../data/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd28fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb85c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from abc import ABC\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"分词器的抽象类，规范了分词器必须要有的方法: `encode()`、`decode()`\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"定义一个 BPETokenizer 所需的全部内容。\n",
    "    意思是有了这些参数：`vocab`、`merges`，就能构建一个 BPE 分词器。\"\"\"\n",
    "    vocab: dict[int, bytes]     # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"遍历所给的 `indices` 列表, 主要作用就是更新索引序列（或者说 Token 序列），\n",
    "    把其中出现的指定 token 对 pair 生成一个新的 token\"\"\"\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        # i + 1 < len(indices) 是用来保证 i 指向的是列表中的第二个 index\n",
    "        # indices[i] == pair[0] and indices[i + 1] == pair[1] ：指定的 token 对 pair\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            # 没有被指定 pair 对的时候，将原来indices中的indice直接添加到new_indices\n",
    "            new_indices.append(indices[i])  \n",
    "            i += 1\n",
    "    return new_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02de5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 99, 99, 4, 99]\n"
     ]
    }
   ],
   "source": [
    "# merge() 举一个例子\n",
    "indices = [1, 2, 3, 2, 3, 4, 2, 3]\n",
    "pair = (2, 3)\n",
    "new_index = 99\n",
    "\n",
    "result = merge(indices, pair, new_index)\n",
    "print(result)  # [1, 99, 99, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"定义一个 BPETokenizer 类，继承了 Tokenizer 类，定义了两个方法：`encode()`, `decode()`\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    '''训练 bpe 分词器，\n",
    "    string: 输入一段字符串\n",
    "    num_merges: 指定进行几次合并\n",
    "    返回类型是 BPETokenizerParams '''\n",
    "    \n",
    "    # 把字符串编码成 UTF-8 字节序列，再转成整型列表。\n",
    "    # map() 的作用是：将 string.encode(\"utf-8\")  的结果转换成整数类型\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    \n",
    "    # merges 定义数据类型是 dict[tuple[int, int], int]，记录的是每次 merge 对应的两个\n",
    "    merges: dict[tuple[int, int], int] = {}\n",
    "    \n",
    "    # 词表 vocab 定义数据类型是 dict[int, bytes]，这里首先初始化了词表\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        \n",
    "        # 统计每一对 token 出现的次数\n",
    "        # defaultdict(int) 的作用是加入字典的键不存在时，自动创建并计数为0\n",
    "        counts = defaultdict(int)\n",
    "        \n",
    "        # 遍历生成相邻两个 token 的组合（index1, index2）\n",
    "        for index1, index2 in zip(indices, indices[1:]):\n",
    "            counts[(index1, index2)] += 1\n",
    "        \n",
    "        # 找到 counts 字典中，值最大的键。这里：如果有多个最大值，返回字典顺序下的第一个。\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "        \n",
    "        # merges 更新\n",
    "        new_index = 256 + i  # i 是从 0 开始的\n",
    "        merges[pair] = new_index\n",
    "        \n",
    "        # 词表更新。两个字节类型的元素相加：不是数值相加，是两个字节拼接到一起\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        \n",
    "        # 更新索引序列\n",
    "        indices = merge(indices, pair, new_index)\n",
    "        \n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab38c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 词表初始化: 256个基础词、特殊 tokens\n",
    "    # 词表 vocab 定义数据类型是 dict[int, bytes]，这里首先初始化了词表\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # 256个基础词，0-255\n",
    "    next_token_id = 256\n",
    "    for special_token in special_tokens:  # 特殊 tokens \n",
    "        # 注意这里所给的特殊 tokens 为 str 格式，vocab 中的是字节形式的\n",
    "        vocab[next_token_id] = special_token.encode(\"utf-8\")\n",
    "        next_token_id += 1\n",
    "    \n",
    "    # 2. 预分词\n",
    "    # 读取数据\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # 预分词规则：gpt2 的分词规则\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    pre_token_matches = re.findall(PAT, content)\n",
    "    \n",
    "    # 统计出现的次数\n",
    "    pre_counts = defaultdict(int)\n",
    "    for match in pre_token_matches:\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. BPE 合并\n",
    "    \n",
    "    \n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595b089",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad escape \\p at position 23",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 匹配单词或数字\u001b[39;00m\n\u001b[0;32m      6\u001b[0m PAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?:[sdmt]|ll|ve|re)| ?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m+| ?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m+| ?[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m pre_token_matches \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello 123, world 456 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\__init__.py:216\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\__init__.py:294\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is an undocumented flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwithout an obvious purpose. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use it.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 294\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_compiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\_compiler.py:743\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    742\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 743\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\_parser.py:980\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    977\u001b[0m state\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m flags\n\u001b[0;32m    978\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m--> 980\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m=\u001b[39m fix_flags(\u001b[38;5;28mstr\u001b[39m, p\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mflags)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source\u001b[38;5;241m.\u001b[39mnext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\_parser.py:455\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    453\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\_parser.py:539\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 539\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[43m_escape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     subpatternappend(code)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SPECIAL_CHARS:\n",
      "File \u001b[1;32mc:\\Users\\OneRaise\\miniconda3\\Lib\\re\\_parser.py:438\u001b[0m, in \u001b[0;36m_escape\u001b[1;34m(source, escape, state)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(escape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ASCIILETTERS:\n\u001b[1;32m--> 438\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbad escape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m escape, \u001b[38;5;28mlen\u001b[39m(escape))\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m LITERAL, \u001b[38;5;28mord\u001b[39m(escape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[1;31merror\u001b[0m: bad escape \\p at position 23"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello 123, world 456 \" *100\n",
    "\n",
    "# 匹配单词或数字\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "pre_token_matches = re.findall(PAT, \"Hello 123, world 456 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32472d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 re.finditer\n",
    "matches = re.finditer(PAT, text)\n",
    "\n",
    "for match in matches:\n",
    "    print(match.group(), \n",
    "          match.start(), \n",
    "          match.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6f60332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' Hello',\n",
       " ' 123',\n",
       " ',',\n",
       " ' world',\n",
       " ' 456',\n",
       " ' ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"Hello 123, world 456 \" * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58754f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findall 结果数量: 2000\n",
      "findall 占用内存大概: 16184 字节\n",
      "finditer 结果数量: 2000\n",
      "finditer 占用内存大概: 48 字节\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "# 假设这是我们的\"语料\"\n",
    "text = \"hello world! \" * 1000\n",
    "\n",
    "pattern = r\"\\w+\"  # 匹配单词\n",
    "\n",
    "# ❌ 方法1：findall（一次性把所有结果放到列表里）\n",
    "tokens_findall = re.findall(pattern, text)\n",
    "print(\"findall 结果数量:\", len(tokens_findall))\n",
    "print(\"findall 占用内存大概:\", sys.getsizeof(tokens_findall), \"字节\")\n",
    "\n",
    "# ✅ 方法2：finditer（生成器，边匹配边用）\n",
    "tokens_finditer = re.finditer(pattern, text)\n",
    "count = 0\n",
    "for match in tokens_finditer:\n",
    "    count += 1\n",
    "print(\"finditer 结果数量:\", count)\n",
    "print(\"finditer 占用内存大概:\", sys.getsizeof(tokens_finditer), \"字节\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9654fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x1ddf01f5330>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37df984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = ['<|endoftext|>']\n",
    "\n",
    "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # 256个基础词，0-255\n",
    "next_token_id = 256\n",
    "for special_token in special_tokens:  # 特殊 tokens \n",
    "    # 注意这里所给的特殊 tokens 为 str 格式，vocab 中的是字节形式的\n",
    "    vocab[next_token_id] = special_token.encode(\"utf-8\")\n",
    "    next_token_id += 1\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17f5b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\\n\\nAfter playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.'}\n",
      "{'text': 'Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\\n\\nRoxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\\n\\nRoxy told Billy about the icy hill and how she couldn\\'t climb it. Billy said, \"I have an idea! Let\\'s find some big leaves to put under your feet. They will help you climb the icy hill.\" Roxy and Billy looked for big leaves and found some. Roxy put the leaves under her feet and tried to climb the icy hill again.\\n\\nThis time, Roxy didn\\'t slip. She climbed and climbed until she reached the top of the icy hill. Roxy was so happy! She and Billy played on the icy hill all day. From that day on, Roxy and Billy were the best of friends, and they climbed and played together all the time. And Roxy learned that with a little help from a friend, she could climb anything.'}\n",
      "{'text': 'Once upon a time, in a small yard, there was a small daisy. The daisy had a name. Her name was Daisy. Daisy was very small, but she was also very happy.\\n\\nOne day, Daisy saw a dog. The dog was big and had a name too. His name was Max. Max liked to play in the yard. Daisy liked to watch Max play. Max and Daisy became friends.\\n\\nEvery day, Max would come to the yard to play. Daisy would watch and smile. They were very happy together. And even though Daisy was small, she knew that she had a big friend in Max.'}\n",
      "{'text': 'Once upon a time, there was a thoughtful girl named Sue. Sue loved to help her mom around the house. One day, her mom asked her to wipe the table after they ate their lunch. Sue was happy to help.\\n\\nAs Sue was wiping the table, she saw a pretty candle on the window sill. The candle was her mom\\'s favorite. Sue wanted to do something nice for her mom, so she said, \"Mom, can I light the candle for you?\" Her mom said, \"Yes, but be very careful.\"\\n\\nSue carefully lit the candle and put it on the table. Her mom was so happy to see the pretty candle. They both sat and watched the candle burn. Sue\\'s mom said, \"Thank you, Sue, for being so thoughtful and careful.\" Sue felt proud that she could help her mom.\\n\\nThe moral of the story is to always be thoughtful and careful when helping others.'}\n",
      "{'text': 'Once upon a time, there was a kind farmer. He had a big cow. The cow was sad. The farmer did not know why.\\n\\nOne day, a little boy came to the farm. He saw the sad cow. The boy kneeled down to talk to the cow. \"Why are you sad, cow?\" he asked. The cow said, \"I am lonely. I want a friend.\"\\n\\nThe kind farmer heard the cow. He wanted to help. So, he got another cow to be friends with the sad cow. The sad cow was happy now. They played together every day. And the kind farmer, the little boy, and the two cows all lived happily ever after.'}\n",
      "{'text': 'Once upon a time, there was a little girl named Lucy. She had a pet cat named Tom. They loved to play together in the big green park near their house. One sunny day, they went to the park to play.\\n\\nWhile playing, Tom saw a big sour lemon on the ground. He wanted to play with it, but when he touched it, it started to roll away. Tom ran after the lemon, trying to catch it. But as he ran, Tom got lost in the park. Lucy looked around, but she could not find Tom. She was very sad.\\n\\nLucy did not give up. She searched the park for her friend. At last, she found him near a big tree. Tom was trying to catch the lemon, but it vanished into a hole in the ground. Tom was happy to see Lucy again. They hugged and went back home together. They had a fun escape in the park, but they decided to leave the sour lemon behind.'}\n",
      "{'text': 'Once upon a time, there was a little brown dog named Spot. He loved to play with his ball in the park. One sunny day, Spot saw a big goal on the other side of the park. He wanted to get his ball into the goal.\\n\\nSpot ran fast with the ball in his mouth. He tried to kick the ball into the goal, but he was too small. Spot started to struggle. He tried again and again, but the ball would not go in.\\n\\nThen, Spot had an idea. He asked his friend, a big brown horse named Buddy, for help. Buddy kicked the ball with his strong legs. The ball flew into the goal! Spot was so happy. He and Buddy played together all day long.'}\n",
      "{'text': 'Once upon a time, there was a little boy named Tom. He loved to play with his red ball. One sunny day, Tom went outside to play with his ball in the land near his home.\\n\\nTom kicked the ball high in the sky. The ball went far, far away. Tom was sad because he could not find his ball. He walked and walked, looking for it. The land was big and sometimes dangerous. Tom knew he had to be careful.\\n\\nAt last, Tom found his ball near a big tree. He was very happy. Tom knew he should not kick the ball too hard next time. He went back home, holding his ball tightly. Tom played safely in his yard, away from the dangerous land.'}\n",
      "{'text': \"Once upon a time, there was a big dog named Max. Max had a red collar that he wore every day. He loved to play and run in the park with his friends.\\n\\nOne day, Max saw a cat on a tree. He wanted to be friends with the cat. So, Max tried to stretch up to reach the cat. But he was not tall enough. He tried again and again, but he just couldn't reach the cat.\\n\\nMax felt sad, but then he had an idea. He found a big box and put it under the tree. Max climbed on the box and stretched one more time. This time, he reached the cat! The cat and Max became good friends, and they played together every day.\"}\n",
      "{'text': 'Once upon a time, there was a girl named Mia. Mia loved her jewelry. She had a big box full of pretty things. She liked to wear them all day. But at night, she had to sleep.\\n\\nOne day, Mia met a talking cat named Tom. Tom was a tough cat, but he was nice. Tom said, \"Hi, Mia! I like your jewelry. Can I wear some too?\" Mia said, \"Yes, Tom. You can wear my jewelry, but we have to give it back before we sleep.\"\\n\\nSo, Mia and Tom played together. They wore the jewelry and had fun. They pretended to be kings and queens. They laughed and danced. But soon, the sun went down, and it was time for bed.\\n\\nMia said, \"Tom, we must give back the jewelry now. It\\'s time to sleep.\" Tom gave back the jewelry and said, \"Thank you, Mia. I had fun today.\" They put the jewelry back in the box and went to sleep. Mia and Tom were happy, and they had sweet dreams.'}\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../data/TinyStories')\n",
    "\n",
    "for i in range(10):\n",
    "    print(dataset['validation'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fb696f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f60c1079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\OneRaise\\\\Desktop\\\\CS336-Notes\\\\assignment1'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train = \"\\n<|endoftext|>\\n\".join(p['text'].replace(\"\\n\\n\", \"\\n\") for p in dataset['train'])\n",
    "# text_valid = \"\\n<|endoftext|>\\n\".join(p['text'].replace(\"\\n\\n\", \"\\n\") for p in dataset['validation'])\n",
    "\n",
    "# # 写入到一个 txt 文件\n",
    "# with open(\"../data/TinyStoriesV2-GPT4-train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(text_train)\n",
    "    \n",
    "# with open(\"../data/TinyStoriesV2-GPT4-valid.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(text_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
