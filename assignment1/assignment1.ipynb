{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e816d8b5",
   "metadata": {},
   "source": [
    "# 草稿1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "150df03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20272702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be395f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\\\x00'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0).__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7040e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'this is a test' + chr(0) + 'string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2a5a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print('this is a test' + chr(0) + 'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6970aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "hello! こんにちは!\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(utf8_encoded.decode(\"utf-8\"))\n",
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150ec1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h 104\n",
      "e 101\n",
      "l 108\n",
      "l 108\n",
      "o 111\n",
      "! 33\n",
      "  32\n",
      "こ 227\n",
      "ん 129\n",
      "に 147\n",
      "ち 227\n",
      "は 130\n",
      "! 147\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_string)):\n",
    "    print(test_string[i], utf8_encoded[i])\n",
    "    \n",
    "print(list(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75a28bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(test_string))\n",
    "print(len(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3de40c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d81d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并保存\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")\n",
    "ds.save_to_disk('../data/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../data/TinyStories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cd28fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09013300",
   "metadata": {},
   "source": [
    "# 草稿2：课程资料上的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from abc import ABC\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6eb85c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(ABC):\n",
    "    \"\"\"分词器的抽象类，规范了分词器必须要有的方法: `encode()`、`decode()`\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"定义一个 BPETokenizer 所需的全部内容。\n",
    "    意思是有了这些参数：`vocab`、`merges`，就能构建一个 BPE 分词器。\"\"\"\n",
    "    vocab: dict[int, bytes]     # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "    def __init__(self, vocab: dict[int, bytes], merges: dict[tuple[int, int], int]):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"遍历所给的 `indices` 列表, 主要作用就是更新索引序列（或者说 Token 序列），\n",
    "    把其中出现的指定 token 对 pair 生成一个新的 token\"\"\"\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        # i + 1 < len(indices) 是用来保证 i 指向的是列表中的第二个 index\n",
    "        # indices[i] == pair[0] and indices[i + 1] == pair[1] ：指定的 token 对 pair\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            # 没有被指定 pair 对的时候，将原来indices中的indice直接添加到new_indices\n",
    "            new_indices.append(indices[i])  \n",
    "            i += 1\n",
    "    return new_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02de5478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 99, 99, 4, 99]\n"
     ]
    }
   ],
   "source": [
    "# merge() 举一个例子\n",
    "indices = [1, 2, 3, 2, 3, 4, 2, 3]\n",
    "pair = (2, 3)\n",
    "new_index = 99\n",
    "\n",
    "result = merge(indices, pair, new_index)\n",
    "print(result)  # [1, 99, 99, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "521c6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer(Tokenizer):\n",
    "    \"\"\"定义一个 BPETokenizer 类，继承了 Tokenizer 类，定义了两个方法：`encode()`, `decode()`\"\"\"\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dcf3780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    '''训练 bpe 分词器，\n",
    "    string: 输入一段字符串\n",
    "    num_merges: 指定进行几次合并\n",
    "    返回类型是 BPETokenizerParams '''\n",
    "    \n",
    "    # 把字符串编码成 UTF-8 字节序列，再转成整型列表。\n",
    "    # map() 的作用是：将 string.encode(\"utf-8\")  的结果转换成整数类型\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    \n",
    "    # merges 定义数据类型是 dict[tuple[int, int], int]，记录的是每次 merge 对应的两个\n",
    "    merges: dict[tuple[int, int], int] = {}\n",
    "    \n",
    "    # 词表 vocab 定义数据类型是 dict[int, bytes]，这里首先初始化了词表\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        \n",
    "        # 统计每一对 token 出现的次数\n",
    "        # defaultdict(int) 的作用是加入字典的键不存在时，自动创建并计数为0\n",
    "        counts = defaultdict(int)\n",
    "        \n",
    "        # 遍历生成相邻两个 token 的组合（index1, index2）\n",
    "        for index1, index2 in zip(indices, indices[1:]):\n",
    "            counts[(index1, index2)] += 1\n",
    "        \n",
    "        # 找到 counts 字典中，值最大的键。这里：如果有多个最大值，返回字典顺序下的第一个。\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "        \n",
    "        # merges 更新\n",
    "        new_index = 256 + i  # i 是从 0 开始的\n",
    "        merges[pair] = new_index\n",
    "        \n",
    "        # 词表更新。两个字节类型的元素相加：不是数值相加，是两个字节拼接到一起\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        \n",
    "        # 更新索引序列\n",
    "        indices = merge(indices, pair, new_index)\n",
    "        \n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32c3d2",
   "metadata": {},
   "source": [
    "# 草稿3：实现bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e9e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from abc import ABC\n",
    "import regex as re\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "from typing import BinaryIO\n",
    "\n",
    "def memory():\n",
    "    '''查看内存占用'''\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"可用内存: {mem.available / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"内存使用率: {mem.percent}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cab38c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_bpe(\n",
    "    input_path: str | os.PathLike,\n",
    "    vocab_size: int,\n",
    "    special_tokens: list[str],\n",
    "    **kwargs,\n",
    ") -> tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "    \"\"\"Given the path to an input corpus, run train a BPE tokenizer and\n",
    "    output its vocabulary and merges.\n",
    "\n",
    "    Args:\n",
    "        input_path (str | os.PathLike): Path to BPE tokenizer training data.\n",
    "        vocab_size (int): Total number of items in the tokenizer's vocabulary (including special tokens).\n",
    "        special_tokens (list[str]): A list of string special tokens to be added to the tokenizer vocabulary.\n",
    "            These strings will never be split into multiple tokens, and will always be\n",
    "            kept as a single token. If these special tokens occur in the `input_path`,\n",
    "            they are treated as any other string.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, bytes], list[tuple[bytes, bytes]]]:\n",
    "            vocab:\n",
    "                The trained tokenizer vocabulary, a mapping from int (token ID in the vocabulary)\n",
    "                to bytes (token bytes)\n",
    "            merges:\n",
    "                BPE merges. Each list item is a tuple of bytes (<token1>, <token2>),\n",
    "                representing that <token1> was merged with <token2>.\n",
    "                Merges are ordered by order of creation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 词表初始化: 256个基础词、特殊 tokens\n",
    "    # 词表 vocab 定义数据类型是 dict[int, bytes]，这里首先初始化了词表\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # 256个基础词，0-255\n",
    "    next_token_id = 256\n",
    "    for special_token in special_tokens:  # 特殊 tokens \n",
    "        # 注意这里所给的特殊 tokens 为 str 格式，vocab 中的是字节形式的\n",
    "        vocab[next_token_id] = special_token.encode(\"utf-8\")\n",
    "        next_token_id += 1\n",
    "    \n",
    "    # 2. 预分词\n",
    "    # 读取数据\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # 预分词规则：gpt2 的分词规则，特殊的 token 使用单独的正则化\n",
    "    special_pat = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "    PAT = rf\"\"\"{special_pat}| '(?:[sdmt]|ll|ve|re)| ?\\p{{L}}+| ?\\p{{N}}+| ?[^\\s\\p{{L}}\\p{{N}}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    pre_token_matches = re.finditer(PAT, content)\n",
    "    \n",
    "    # 统计出现的次数\n",
    "    pre_indices = defaultdict(int)\n",
    "    for pre_token_matche in pre_token_matches:\n",
    "        pre_indices_key = tuple(map(int, pre_token_matche.group().encode('utf-8')))\n",
    "        pre_indices[pre_indices_key] += 1\n",
    "    \n",
    "    # 3. BPE 合并\n",
    "    # 合并次数为词表大小\n",
    "    num_merges = vocab_size - 256 - len(special_tokens)\n",
    "    merges: list[tuple[bytes, bytes]] = []\n",
    "    indices = pre_indices\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        counts = defaultdict(int)\n",
    "        # indice 字典中的 key\n",
    "        for index in indices:\n",
    "            # 生成相邻两个 token 的组合（index1, index2）\n",
    "            for index1, index2 in zip(index, index[1:]):\n",
    "                # indices[indice] 为合并前（index1, index2）出现的次数\n",
    "                counts[(index1, index2)] += indices[index]\n",
    "        # 找到出现次数最多的\n",
    "        # pair = max(counts, key=counts.get)\n",
    "        # 1. 先找出最大的值\n",
    "        max_val = max(counts.values())\n",
    "        # 2. 找出值等于最大值的所有键，然后max()找到字典序最大的合并\n",
    "        pair = max([k for k, v in counts.items() if v == max_val])\n",
    "        \n",
    "        # merges 更新\n",
    "        merges.append(pair)\n",
    "        index1, index2 = pair\n",
    "        \n",
    "        # 词表更新。两个字节类型的元素相加：不是数值相加，是两个字节拼接到一起\n",
    "        vocab[next_token_id] = vocab[index1] + vocab[index2]\n",
    "        \n",
    "        # 更新 indices 字典\n",
    "        new_indices = defaultdict(int)\n",
    "        for index in indices:\n",
    "            index_value = indices[index]\n",
    "            new_index = []\n",
    "            i = 0\n",
    "            while i < len(index):\n",
    "                # i + 1 < len(index) 是用来保证 i 指向的是列表中的第二个 index\n",
    "                # index[i] == pair[0] and index[i + 1] == pair[1] ：指定的 token 对 pair\n",
    "                if i + 1 < len(index) and index[i] == pair[0] and index[i + 1] == pair[1]:\n",
    "                    new_index.append(next_token_id)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # 没有被指定 pair 对的时候，将原来index中的indice直接添加到new_index\n",
    "                    new_index.append(index[i])  \n",
    "                    i += 1\n",
    "            new_indices[tuple(new_index)] = index_value\n",
    "\n",
    "        # 这里才能将 next_token_id 加一，因为在更新 indices 字典时，会用到 next_token_id\n",
    "        next_token_id += 1\n",
    "        indices = new_indices\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c48a5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merges = run_train_bpe('../data/TinyStoriesV2-GPT4-valid.txt', 300, ['<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 草稿\n",
    "\n",
    "special_tokens = ['<|endoftext|>']\n",
    "\n",
    "merges: list[tuple[bytes, bytes]] = []\n",
    "vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}\n",
    "next_token_id = 256\n",
    "for special_token in special_tokens:  # 特殊 tokens \n",
    "    # 注意这里所给的特殊 tokens 为 str 格式，vocab 中的是字节形式的\n",
    "    vocab[next_token_id] = special_token.encode(\"utf-8\")\n",
    "    next_token_id += 1\n",
    "    \n",
    "with open('../data/text_example.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()[:10000]\n",
    "    \n",
    "pre_indices = defaultdict(int)\n",
    "\n",
    "special_pat = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "PAT = rf\"\"\"{special_pat}| '(?:[sdmt]|ll|ve|re)| ?\\p{{L}}+| ?\\p{{N}}+| ?[^\\s\\p{{L}}\\p{{N}}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "pre_token_matches = re.finditer(PAT, content)\n",
    "\n",
    "for pre_token_matche in pre_token_matches:\n",
    "    pre_counts_key = tuple(map(int, pre_token_matche.group().encode('utf-8')))\n",
    "    pre_indices[pre_counts_key] += 1\n",
    "\n",
    "\n",
    "indices = pre_indices.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de24032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: {(108, 111): 70, (111, 119): 70, (32, 108): 50, (119, 101): 80, (101, 114): 20, (32, 119): 30, (119, 105): 30, (105, 100): 30, (100, 101): 30, (101, 257): 90, (110, 101): 60, (101, 119): 60, (32, 110): 50, (60, 124): 10, (124, 101): 10, (101, 110): 10, (110, 100): 10, (100, 111): 10, (111, 102): 10, (102, 116): 10, (116, 101): 10, (101, 120): 10, (120, 116): 10, (116, 124): 10, (124, 62): 10}\n",
      "本次需要合并的 token： (101, 257)\n",
      "更新前next_token_id: 258\n",
      "更新后next_token_id: 258\n",
      "词表的大小： 259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'<|endoftext|>',\n",
       " 257: b'st',\n",
       " 258: b'est'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "counts = defaultdict(int)\n",
    "\n",
    "\n",
    "# indice 字典中的 key\n",
    "for indice in indices:\n",
    "    # 生成相邻两个 token 的组合（index1, index2）\n",
    "    for index1, index2 in zip(indice, indice[1:]):\n",
    "        # indices[indice] 为合并前（index1, index2）出现的次数\n",
    "        counts[(index1, index2)] += indices[indice]\n",
    "print('counts:', dict(counts))\n",
    "# 找到出现次数最多的\n",
    "# pair = max(counts, key=counts.get)  # 这种方式不行！用下面的方法\n",
    "# 1. 先找出最大的值\n",
    "max_val = max(counts.values())\n",
    "# 2. 找出值等于最大值的所有键，然后max()找到字典序最大的合并\n",
    "pair = max([k for k, v in counts.items() if v == max_val])\n",
    "\n",
    "# 添加到 merges 中\n",
    "merges.append(pair)\n",
    "index1, index2 = pair\n",
    "print('本次需要合并的 token：', pair)\n",
    "print('更新前next_token_id:', next_token_id)\n",
    "\n",
    "\n",
    "# 词表更新。两个字节类型的元素相加：不是数值相加，是两个字节拼接到一起\n",
    "# print(vocab[next_token_id])\n",
    "vocab[next_token_id] = vocab[index1] + vocab[index2]\n",
    "\n",
    "print('更新后next_token_id:', next_token_id)\n",
    "print('词表的大小：', len(vocab))\n",
    "\n",
    "\n",
    "# 更新 indices 字典\n",
    "new_indices = defaultdict(int)\n",
    "for index in indices:\n",
    "    index_value = indices[index]\n",
    "    new_index = []\n",
    "    i = 0\n",
    "    while i < len(index):\n",
    "        # i + 1 < len(index) 是用来保证 i 指向的是列表中的第二个 index\n",
    "        # index[i] == pair[0] and index[i + 1] == pair[1] ：指定的 token 对 pair\n",
    "        if i + 1 < len(index) and index[i] == pair[0] and index[i + 1] == pair[1]:\n",
    "            new_index.append(next_token_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            # 没有被指定 pair 对的时候，将原来index中的indice直接添加到new_index\n",
    "            new_index.append(index[i])  \n",
    "            i += 1\n",
    "    new_indices[tuple(new_index)] = index_value\n",
    "\n",
    "# 这里才能将 next_token_id 加一，因为在更新 indices 字典时，会用到 next_token_id\n",
    "next_token_id += 1\n",
    "indices = new_indices\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a83be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(108, 111): 70,\n",
       "             (111, 119): 70,\n",
       "             (32, 108): 50,\n",
       "             (119, 101): 80,\n",
       "             (101, 114): 20,\n",
       "             (32, 119): 30,\n",
       "             (119, 105): 30,\n",
       "             (105, 100): 30,\n",
       "             (100, 101): 30,\n",
       "             (101, 115): 90,\n",
       "             (115, 116): 90,\n",
       "             (110, 101): 60,\n",
       "             (101, 119): 60,\n",
       "             (32, 110): 50,\n",
       "             (60, 124): 10,\n",
       "             (124, 101): 10,\n",
       "             (101, 110): 10,\n",
       "             (110, 100): 10,\n",
       "             (100, 111): 10,\n",
       "             (111, 102): 10,\n",
       "             (102, 116): 10,\n",
       "             (116, 101): 10,\n",
       "             (101, 120): 10,\n",
       "             (120, 116): 10,\n",
       "             (116, 124): 10,\n",
       "             (124, 62): 10})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef1e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_indices[(105, 110)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "12b04370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((110, 105), 70)\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    (108, 111): 70,\n",
    "    (109, 110): 70,\n",
    "    (100, 200): 60,\n",
    "    (110, 105): 70\n",
    "}\n",
    "\n",
    "# 1. 找出最大值\n",
    "max_val = max(data.values())\n",
    "\n",
    "# 2. 找出值等于最大值的所有键\n",
    "candidates = [k for k, v in data.items() if v == max_val]\n",
    "\n",
    "# 3. 找出键最大的那个元组\n",
    "max_key = max(candidates)\n",
    "\n",
    "# 4. 得到最终结果\n",
    "result = (max_key, max_val)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9f68641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 105)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([k for k, v in data.items() if v == max_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "651f299c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(108, 111), (109, 110), (110, 105)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "977f094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用内存: 3132.28 MB\n",
      "内存使用率: 77.9%\n"
     ]
    }
   ],
   "source": [
    "memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "602bec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {(108, 111): 70,\n",
       "             (111, 119): 70,\n",
       "             (32, 108): 50,\n",
       "             (119, 101): 80,\n",
       "             (101, 114): 20,\n",
       "             (32, 119): 30,\n",
       "             (119, 105): 30,\n",
       "             (105, 100): 30,\n",
       "             (100, 101): 30,\n",
       "             (101, 115): 90,\n",
       "             (115, 116): 90,\n",
       "             (110, 101): 60,\n",
       "             (101, 119): 60,\n",
       "             (32, 110): 50,\n",
       "             (60, 124): 10,\n",
       "             (124, 101): 10,\n",
       "             (101, 110): 10,\n",
       "             (110, 100): 10,\n",
       "             (100, 111): 10,\n",
       "             (111, 102): 10,\n",
       "             (102, 116): 10,\n",
       "             (116, 101): 10,\n",
       "             (101, 120): 10,\n",
       "             (120, 116): 10,\n",
       "             (116, 124): 10,\n",
       "             (124, 62): 10})"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "36fd9e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 115)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(counts, key=counts.get)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
