{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973162f8",
   "metadata": {},
   "source": [
    "# CS336 Language Modeling from Scratch 从零开始构建语言模型\n",
    "\n",
    "- 课程简介: \\\n",
    "  语言模型开启了一个全新的范式————通过一个通用系统来解决各种**下游任务**。本课程将介绍语言模型创建的各个环节————数据收集与清洗、Transformer 模型的构建、模型训练，以及部署前的评估。\n",
    "  > **下游任务**：\n",
    "  > 指利用已经训练好的语言模型在具体应用场景中执行的各种自然语言处理任务。例如文本分类、命名实体识别（NER，例如识别人名、地名、组织名等）、问答系统（如基于知识库或文档的问答）、文本生成（如对话生成、文章续写、代码生成、文本摘要（生成文章或文档的简要版本）、机器翻译（如英文翻译成中文）、语义相似度判断（判断两段文本是否语义相近）、文本纠错与语法改进等\n",
    "- 先决条件：\\\n",
    "  python，深度学习与系统优化，微积分与线性代数，概率与统计，机器学习基础\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff52032",
   "metadata": {},
   "source": [
    "    welcome()\n",
    "    why_this_course_exists()\n",
    "    current_landscape()\n",
    "    what_is_this_program()\n",
    "    course_logistics()\n",
    "    course_components()\n",
    "        Overview of the course\n",
    "        basics()\n",
    "        systems()\n",
    "        scaling_laws()\n",
    "        data()\n",
    "        alignment()\n",
    "    tokenization()\n",
    "        intro_to_tokenization()\n",
    "        tokenization_examples()\n",
    "        character_tokenizer()\n",
    "        byte_tokenizer()\n",
    "        word_tokenizer()\n",
    "        bpe_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6165e",
   "metadata": {},
   "source": [
    "## 开设这门课的原因\n",
    "\n",
    "科研人员正逐渐与底层技术脱节（自己实现模型$\\rightarrow$对现有模型微调$\\rightarrow$写提示词），但是基础研究必须深入了解底层技术才能完成。本课程的目标就是通过**搭建**语言模型来深入理其深层原理。\n",
    "\n",
    "## 工业界的语言模型\n",
    "GPT-4 拥有 1.8 万亿个参数，训练成本高达 1 亿美金；xAI 使用 20 万张 H100 GPU 组建集群，用于训练 Grok 模型；Stargate 项目计划在 4 年内投资 5000 亿美元。\n",
    "> [星际之门计划（Stargate Project）](https://openai.com/index/announcing-the-stargate-project/)是美国一家人工智能合资企业，由OpenAI 、软银、甲骨文和投资公司 MGX 联合创立。该合资公司计划到2029年在美国的人工智能基础设施上投资高达**5000亿美元**。这一计划于2022年启动，并在2025年1月21日由美国总统唐纳德·川普正式宣布。该公司在美国特拉华州注册成立，名称为Stargate LLC。软银首席执行官孙正义被任命为公司董事长。\n",
    "\n",
    "## 模型更大的时候会有所不同\n",
    "前沿的模型对我们来说遥不可及，本课程中构建的小语言模型无法完全代表大预言模型的真实特征。\n",
    "- 例1：随着模型规模扩大，注意力机制在计算中所占的比例迅速减少，而前馈层变成了真正的主力计算负担。\n",
    "  \n",
    "  ![](images\\roller-flops.png)\n",
    "\n",
    "  | 列名                | 含义                                  |\n",
    "  | ----------------| ---------------------------------------- |\n",
    "  | **description** | 模型的规模（参数数量）                     |\n",
    "  | **FLOPs/update**| 每次参数更新所需的总浮点运算次数（FLOPs）    |\n",
    "  | **FLOPS MHA**   | 多头注意力（Multi-Head Attention）部分占总 FLOPs 的百分比 |\n",
    "  | **FLOPS FFN**   | 前馈神经网络（Feed-Forward Network）部分的 FLOPs 占比     |\n",
    "  | **FLOPS attn**  | 计算注意力权重（attention scores）的 FLOPs 占比           |\n",
    "  | **FLOPS logit** | 输出 logits（即分类器部分）的 FLOPs 占比                  |\n",
    "\n",
    "\n",
    "- 例2：随着**模型规模**的增加，模型会在某些任务上突然出现**能力涌现（emergent abilities）**。\n",
    "\n",
    "  ![](images\\wei-emergence-plot.png)\n",
    "\n",
    "## 这门课上能学到的能够迁移到前沿模型的知识\n",
    "- 运行机制：Transformer原理、模型并行利用GPU\n",
    "- 思维方式：如何最大限度地榨干硬件性能、认真对待规模问题（例如规模法则scaling laws）\n",
    "- 直觉经验：在数据选择和建模决策中，哪些因素有助于提高准确率\n",
    "机制和思维方式是可以迁移到大型模型上的，而直觉经验在不同规模下并不总是适用的。\n",
    "> Scaling laws（规模法则）：随着模型的规模、训练数据量和计算量的增长，模型性能如何系统性地提升。这些法则是通过大量实验观察总结出来的经验规律，特别适用于神经网络、尤其是语言模型的发展。经典 scaling law：有研究表明，模型损失（loss）通常近似满足如下关系：\\\n",
    "> $\\text{Loss} \\propto (\\text{Compute})^{-\\alpha}$ \\\n",
    "> 其中 α 是经验上观察到的正数（通常 0.05–0.1 左右），意味着计算量每增加一个数量级，损失会稳步下降。\\\n",
    "\n",
    ">  | 增长项    | 对性能的影响                |\n",
    ">  | ------ | --------------------- |\n",
    ">  | 参数量增加  | 性能提升，但回报逐渐递减          |\n",
    ">  | 数据量增加  | 初期提升显著，但过多无新信息会饱和     |\n",
    ">  | 训练步骤增加 | 可以进一步榨干模型能力，但受限于过拟合风险 |\n",
    ">  | 计算资源增加 | 性能提升，但需平衡投入与回报        |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
